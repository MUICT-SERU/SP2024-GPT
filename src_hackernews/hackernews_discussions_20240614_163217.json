[
    {
        "by": "FabioFleitas",
        "id": 40672027,
        "score": 1,
        "time": 1718298056,
        "title": "Tesorio (YC S15) Is Hiring a Senior GenAI/LLM Engineer (100% Remote)",
        "type": "job",
        "url": "https://www.tesorio.com/careers#job-openings",
        "kids_content": []
    },
    {
        "by": "mizzao",
        "descendants": 242,
        "id": 40665721,
        "kids": [
            40668263,
            40669810,
            40667983,
            40666893,
            40666023,
            40671732,
            40667926,
            40666684,
            40666249,
            40667873,
            40665987,
            40675375,
            40666492,
            40666198,
            40670539,
            40666128,
            40672419,
            40668177,
            40666038,
            40666012,
            40666019,
            40666313,
            40666953,
            40666318,
            40670949,
            40666395,
            40666255
        ],
        "score": 495,
        "time": 1718250159,
        "title": "Uncensor any LLM with abliteration",
        "type": "story",
        "url": "https://huggingface.co/blog/mlabonne/abliteration",
        "kids_content": [
            {
                "by": "rivo",
                "id": 40668263,
                "kids": [
                    40669447,
                    40671323,
                    40668938,
                    40669291
                ],
                "parent": 40665721,
                "text": "I tried the model the article links to and it was so refreshing not being denied answers to my questions. It even asked me at the end &quot;Is this a thought experiment?&quot;, I replied with &quot;yes&quot;, and it said &quot;It&#x27;s fun to think about these things, isn&#x27;t it?&quot;<p>It felt very much like hanging out with your friends, having a few drinks, and pondering big, crazy, or weird scenarios. Imagine your friend saying, &quot;As your friend, I cannot provide you with this information.&quot; and completely ruining the night. That&#x27;s not going to happen. Even my kids would ask me questions when they were younger: &quot;Dad, how would you destroy earth?&quot; It would be of no use to anybody to deny answering that question. And answering them does not mean they will ever attempt anything like that. There&#x27;s a reason Randall Munroe&#x27;s &quot;What If?&quot; blog became so popular.<p>Sure, there are dangers, as others are pointing out in this thread. But I&#x27;d rather see disclaimers (&quot;this may be wrong information&quot; or &quot;do not attempt&quot;) than my own computer (or the services I pay for) straight out refusing my request.",
                "time": 1718277944,
                "type": "comment"
            },
            {
                "by": "giancarlostoro",
                "id": 40669810,
                "kids": [
                    40676828,
                    40670109,
                    40671835,
                    40670220,
                    40671863
                ],
                "parent": 40665721,
                "text": "I&#x27;ve got friends who tried to use ChatGPT to generate regex to capture racial slurs to moderate them (perfectly valid request since they&#x27;re trying to stop trolls from saying awful things). It vehemently refused to do so, probably due to overtly strict &quot;I&#x27;ll never say the nword, you can&#x27;t fool me&quot; rules that were shoved into ChatGPT. Look, if your AI can&#x27;t be intelligent about sensible requests, I&#x27;m going to say it. It&#x27;s not intelligent, it&#x27;s really useless (at least regarding that task, and related valid tasks).<p>Who cares if someone can get AI to say awful things? I can write software that spits out slurs without the help of AI. Heck, I could write awful things here on HN, is AI going to stop me? Doubt it, nobody wants to foot the bill for AI moderation, it can only get so much.",
                "time": 1718287512,
                "type": "comment"
            },
            {
                "by": "YukiElectronics",
                "id": 40667983,
                "kids": [
                    40669226,
                    40668220,
                    40676978
                ],
                "parent": 40665721,
                "text": "&gt; Once we have identified the refusal direction, we can &quot;ablate&quot; it, effectively removing the model&#x27;s ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization.<p>Finally, even a LLM can get lobotomised",
                "time": 1718275277,
                "type": "comment"
            },
            {
                "by": "k__",
                "id": 40666893,
                "kids": [
                    40667339,
                    40669069,
                    40667091,
                    40666950,
                    40669289,
                    40671251,
                    40669327
                ],
                "parent": 40665721,
                "text": "I played around with Amazon Q and while setting it up, I needed to create an IAM identity center.<p>Never did this before, so I was asking Q in the AWS docs how to do it.<p>It refused to help, as it didn&#x27;t answer security related questions.<p>thank.",
                "time": 1718263273,
                "type": "comment"
            },
            {
                "by": "schoen",
                "id": 40666023,
                "kids": [
                    40666319
                ],
                "parent": 40665721,
                "text": "This is really interesting and is parallel to some other stuff (like the research on a model that&#x27;s obsessed with the Golden Gate Bridge and inappropriately thinks of things related to it in otherwise irrelevant contexts).<p>It&#x27;s worth mentioning that this technique is usable <i>if you have the model weights</i> (it&#x27;s a simple way of changing the weights or how to use them):<p>&gt; Once we have identified the refusal direction, we can &quot;ablate&quot; it, effectively removing the model&#x27;s ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization.<p>It&#x27;s not (and doesn&#x27;t claim to be) a technique for convincing a model to change its behavior <i>through prompts</i>.",
                "time": 1718253729,
                "type": "comment"
            },
            {
                "by": "throwaway4aday",
                "id": 40671732,
                "parent": 40665721,
                "text": "Holy buried lede Batman! Right at the end.<p>&gt; Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy&#x27;s MopeyMule, which adopts a melancholic conversational style.<p><a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;failspy&#x2F;Llama-3-8B-Instruct-MopeyMule\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;failspy&#x2F;Llama-3-8B-Instruct-MopeyMule</a><p>Finally! We have discovered the recipe to produce Genuine People Personalities!",
                "time": 1718296419,
                "type": "comment"
            },
            {
                "by": "olalonde",
                "id": 40667926,
                "kids": [
                    40668086,
                    40669086,
                    40667968,
                    40668163,
                    40670974
                ],
                "parent": 40665721,
                "text": "&gt; Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests.<p>It&#x27;s sad that it&#x27;s now an increasingly accepted idea that information one seeks can be &quot;harmful&quot;.",
                "time": 1718274703,
                "type": "comment"
            },
            {
                "by": "vasco",
                "id": 40666684,
                "kids": [
                    40666984,
                    40666709,
                    40667025,
                    40666890,
                    40666835,
                    40667243,
                    40667633,
                    40670809,
                    40669842,
                    40666992,
                    40666828
                ],
                "parent": 40665721,
                "text": "&gt; &quot;As an AI assistant, I cannot help you.&quot; While this safety feature is crucial for preventing misuse,<p>What is the safety added by this? What is unsafe about a computer giving you answers?",
                "time": 1718261351,
                "type": "comment"
            },
            {
                "by": "Mathnerd314",
                "id": 40666249,
                "kids": [
                    40666307
                ],
                "parent": 40665721,
                "text": "Reminds me of <a href=\"https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;</a>. There they were adding a control vector, w&#x27; = cvec + w, here they are &quot;ablating&quot; it, \nw&#x27; = w - dot(w,cvec)*cvec. There is an interesting field of learning how to &quot;brain chip&quot; LLMs into doing what you want.",
                "time": 1718256520,
                "type": "comment"
            },
            {
                "by": "TeMPOraL",
                "id": 40667873,
                "parent": 40665721,
                "text": "Normally I&#x27;d call this lobotomizing the AI, and I&#x27;ve been worried for a while this is how models will become further shackled by the vendors operating them. In this case, however, it feels more like <i>deprogramming</i>, which is something I can get behind. I didn&#x27;t expect the line between the two to be so blurry, though in retrospect it&#x27;s obvious that the same technique can be used for both.",
                "time": 1718274129,
                "type": "comment"
            }
        ]
    },
    {
        "by": "drchiu",
        "descendants": 1,
        "id": 40674861,
        "kids": [
            40674886
        ],
        "score": 18,
        "time": 1718313330,
        "title": "Apple Reportedly Not Paying OpenAI to Use ChatGPT in iOS 18",
        "type": "story",
        "url": "https://www.macrumors.com/2024/06/13/apple-not-paying-openai-chatgpt-ios-18/",
        "kids_content": [
            {
                "by": "gnabgib",
                "id": 40674886,
                "parent": 40674861,
                "text": "Discussion [0] (53 points, 12 hours ago, 67comments)<p>[0]: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40667416\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40667416</a>",
                "time": 1718313469,
                "type": "comment"
            }
        ]
    },
    {
        "by": "typpo",
        "descendants": 2,
        "id": 40671686,
        "kids": [
            40672026,
            40673973
        ],
        "score": 20,
        "text": "Hi HN,<p>I built this open-source LLM red teaming tool based on my experience scaling LLMs at a big co to millions of users... and seeing all the bad things people did.<p>How it works:<p>- Uses an unaligned model to create toxic inputs<p>- Runs these inputs through your app using different techniques: raw, prompt injection, and a chain-of-thought jailbreak that tries to re-frame the request to trick the LLM.<p>- Probes a bunch of other failure cases (e.g. will your customer support bot recommend a competitor? Does it think it can process a refund when it can&#x27;t?  Will it leak your user&#x27;s address?)<p>- Built on top of promptfoo, a popular eval tool<p>One interesting thing about my approach is that almost none of the tests are hardcoded.  They are all tailored toward the specific purpose of your application, which makes the attacks more potent.<p>Some of these tests reflect fundamental, unsolved issues with LLMs.  Other failures can be solved pretty trivially by prompting or safeguards.<p>Most businesses will never ship LLMs without at least being able to quantify these types of risks.  So I hope this helps someone out.  Happy building!",
        "time": 1718296159,
        "title": "Show HN: Automated red teaming for your LLM app",
        "type": "story",
        "url": "https://www.promptfoo.dev/docs/red-team/",
        "kids_content": [
            {
                "by": "danenania",
                "id": 40672026,
                "parent": 40671686,
                "text": "I haven&#x27;t yet tried this red teaming tool, but I recently started using promptfoo to build out an evals pipeline for Plandex, a terminal-based AI coding tool I&#x27;m building[1]. promptfoo has been a pleasure to work with so far and I&#x27;d  recommend it to anyone who knows they need evals but isn&#x27;t sure where to begin.<p>It&#x27;s quite flexible for different kinds of prompting scenarios and makes it easy to e.g. test a prompt n number of times (good for catching long-tail issues), only re-run evals that failed previously (helps to reduce costs&#x2F;running time when you&#x27;re iterating), or define various kinds of success criteria--exactly matches an expected string, contains an expected substring, a boolean JSON property is true&#x2F;false, an LLM call that determines success, etc. etc. It pretty much covers all the bases on that front.<p>It can also treat prompts as jinja2 templates which is good for testing &#x27;dynamic&#x27; prompts which take parameters (all of Plandex&#x27;s prompts are like this).<p>It seems like a good foundation to build red teaming on top of.<p>1 - <a href=\"https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex\">https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex</a>",
                "time": 1718298051,
                "type": "comment"
            },
            {
                "by": "Oras",
                "id": 40673973,
                "parent": 40671686,
                "text": "Can this be dynamic on prompts and providers?<p>I\u2019m thinking of continuous evaluation for LLM in production, where after each call, a webhook will send the input&#x2F;output to evaluate.",
                "time": 1718308763,
                "type": "comment"
            }
        ]
    }
]