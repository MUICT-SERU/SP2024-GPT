{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KIDS = 100 # per story\n",
    "\n",
    "HN_STORIES_DATASET_FILENAME = '../hn_stories_dataset.json'\n",
    "HN_COMMENTS_DATASET_FILENAME = '../hn_comments_dataset.json'\n",
    "\n",
    "KEYWORDS_FILENAME = '../ai_keywords.txt'\n",
    "\n",
    "# chatgpt_gh_filename = 'github_links_chatgpt.json'\n",
    "\n",
    "# Define the base URL for the Hacker News API\n",
    "BASE_URL = 'https://hacker-news.firebaseio.com/v0'\n",
    "\n",
    "# CHATGPT_RELEASE_ID = 33804874\n",
    "# START_ID = 31300000 # may 8th 2022\n",
    "# END_ID = 40300000 # may 9th 2024\n",
    "\n",
    "DEPTH = 2 # comments depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_top_story_ids(session):\n",
    "    async with session.get(f'{BASE_URL}/topstories.json') as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def get_item(session, item_id):\n",
    "    async with session.get(f'{BASE_URL}/item/{item_id}.json') as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def get_kids_hierarchical(session, item, depth=DEPTH):\n",
    "    if 'kids' not in item or depth <= 0:\n",
    "        return []\n",
    "\n",
    "    kids_hierarchy = []\n",
    "    tasks = []\n",
    "    for kid_id in item['kids'][:NUM_KIDS]:\n",
    "        tasks.append(get_item(session, kid_id))\n",
    "\n",
    "    kids = await asyncio.gather(*tasks)\n",
    "    for kid in kids:\n",
    "        if kid:\n",
    "            kid_data = {\n",
    "                'id': kid.get('id'),\n",
    "                'text': kid.get('text'),\n",
    "                'time': kid.get('time'),\n",
    "                'author': kid.get('by'),\n",
    "                'depth': DEPTH - depth + 1,\n",
    "                'children': await get_kids_hierarchical(session, kid, depth - 1)\n",
    "            }\n",
    "            kids_hierarchy.append(kid_data)\n",
    "    return kids_hierarchy\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'processed_story_max_id': -1, 'stories': []}\n",
    "\n",
    "def save_progress(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "async def retrieve_stories_comments(stories_source_filename, comments_dest_filename):\n",
    "    stories_dest = load_progress(comments_dest_filename)\n",
    "    progress_story_max_id = stories_dest['processed_story_max_id']\n",
    "    stories_comments_dest = stories_dest['stories']\n",
    "    curr_num_stories = len(stories_comments_dest)\n",
    "\n",
    "    stories_source = load_progress(stories_source_filename)\n",
    "    total_num_stories = len(stories_source['stories'])\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        pbar = tqdm(total=total_num_stories, desc=\"Fetching comments from stories\")\n",
    "        pbar.update(curr_num_stories)\n",
    "\n",
    "        for story in stories_source['stories']:\n",
    "            try:\n",
    "                if story['id'] <= progress_story_max_id:\n",
    "                    continue\n",
    "\n",
    "                story['kids_text'] = await get_kids_hierarchical(session, story, depth=DEPTH)\n",
    "                stories_comments_dest.append(story)\n",
    "\n",
    "                pbar.update(1)\n",
    "                progress_story_max_id = story['id']\n",
    "                save_progress({'processed_story_max_id': progress_story_max_id,\n",
    "                               'stories': stories_comments_dest}, comments_dest_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing story {story['id']}: {e}\")\n",
    "                save_progress({'processed_story_max_id': progress_story_max_id,\n",
    "                               'stories': stories_comments_dest}, comments_dest_filename)\n",
    "        pbar.close()\n",
    "    return stories_comments_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x00000207B6066290>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\prach\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\prach\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the original JSON file\n",
    "    with open(HN_STORIES_DATASET_FILENAME, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Process the stories and fetch comments\n",
    "    processed_data = await retrieve_stories_comments(HN_STORIES_DATASET_FILENAME, HN_COMMENTS_DATASET_FILENAME)\n",
    "\n",
    "    print('Done')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stories: 399\n",
      "Total number of comments: 14837\n"
     ]
    }
   ],
   "source": [
    "def count_comments(data):\n",
    "    return len(data), sum(len(story['kids_text']) for story in data)\n",
    "\n",
    "num_stories, num_comments = count_comments(processed_data)\n",
    "print(f\"Total number of stories: {num_stories}\")\n",
    "print(f\"Total number of comments: {num_comments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import html\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_github_urls(text):\n",
    "    github_url_pattern = r\"https://github\\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+(?:/(?:issues|pull)/\\d+)?\"\n",
    "    return list(set(re.findall(github_url_pattern, text)))\n",
    "\n",
    "def is_github_repo_or_issue(url):\n",
    "    if not url.startswith(\"https://github.com/\"):\n",
    "        return \"Not GitHub\"\n",
    "    if \"/issues/\" in url:\n",
    "        return \"GitHub Issue\"\n",
    "    if \"/pull/\" in url:\n",
    "        return \"GitHub PR\"\n",
    "    return \"GitHub Repo\"\n",
    "\n",
    "def unix_to_datetime(unix_timestamp):\n",
    "    return datetime.fromtimestamp(unix_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def clean_html(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def flatten_comments(comments, discussion_id, title, url, discussion_date, parent_id=None, depth=0):\n",
    "    flattened = []\n",
    "    for comment in comments:\n",
    "        comment_id = comment.get('id', '')\n",
    "        comment_text = clean_html(comment.get('text', ''))\n",
    "        comment_date = unix_to_datetime(comment.get('time', 0))\n",
    "        comment_author = comment.get('author', '')\n",
    "\n",
    "        flattened.append({\n",
    "            'discussion_id': discussion_id,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'discussion_date': discussion_date,\n",
    "            'comment_id': comment_id,\n",
    "            'parent_id': parent_id,\n",
    "            'depth': depth,\n",
    "            'comment_text': comment_text,\n",
    "            'comment_date': comment_date,\n",
    "            'comment_author': comment_author,\n",
    "        })\n",
    "\n",
    "        if 'children' in comment:\n",
    "            flattened.extend(flatten_comments(comment['children'], discussion_id, title, url, discussion_date, comment_id, depth + 1))\n",
    "\n",
    "    return flattened\n",
    "\n",
    "def process_stories(discussions):\n",
    "    rows = []\n",
    "    github_urls = set()\n",
    "\n",
    "    for discussion in discussions['stories']:\n",
    "        discussion_id = discussion.get('id')\n",
    "        title = discussion.get('title', '')\n",
    "        url = discussion.get('url', '')\n",
    "        discussion_date = unix_to_datetime(discussion.get('time', 0))\n",
    "\n",
    "        github_urls.update(extract_github_urls(title))\n",
    "        github_urls.update(extract_github_urls(url))\n",
    "\n",
    "        if 'comments_hierarchy' in discussion:\n",
    "            rows.extend(flatten_comments(discussion['comments_hierarchy'], discussion_id, title, url, discussion_date))\n",
    "        else:\n",
    "            rows.append({\n",
    "                'discussion_id': discussion_id,\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'discussion_date': discussion_date,\n",
    "                'comment_id': '',\n",
    "                'parent_id': '',\n",
    "                'depth': 0,\n",
    "                'comment_text': '',\n",
    "                'comment_date': '',\n",
    "                'comment_author': '',\n",
    "            })\n",
    "\n",
    "    return rows, github_urls\n",
    "\n",
    "def write_csv(filename, data, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m chatgpt_data \u001b[38;5;241m=\u001b[39m read_json_file(HN_COMMENTS_DATASET_FILENAME)\n\u001b[1;32m----> 3\u001b[0m comments_rows, comments_github_urls \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_stories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchatgpt_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m fieldnames \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscussion_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_text\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n\u001b[0;32m     13\u001b[0m write_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchatgpt_comments.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, comments_rows, fieldnames)\n",
      "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36mprocess_stories\u001b[1;34m(discussions)\u001b[0m\n\u001b[0;32m     69\u001b[0m github_urls\u001b[38;5;241m.\u001b[39mupdate(extract_github_urls(url))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments_hierarchy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m discussion:\n\u001b[1;32m---> 72\u001b[0m     rows\u001b[38;5;241m.\u001b[39mextend(\u001b[43mflatten_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscussion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments_hierarchy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscussion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscussion_date\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscussion_id\u001b[39m\u001b[38;5;124m'\u001b[39m: discussion_id,\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: title,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment_author\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     85\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mflatten_comments\u001b[1;34m(comments, discussion_id, title, url, discussion_date, parent_id, depth)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comments:\n\u001b[0;32m     35\u001b[0m     comment_id \u001b[38;5;241m=\u001b[39m comment\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     comment_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     comment_date \u001b[38;5;241m=\u001b[39m unix_to_datetime(comment\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     38\u001b[0m     comment_author \u001b[38;5;241m=\u001b[39m comment\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mclean_html\u001b[1;34m(html_text)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_html\u001b[39m(html_text):\n\u001b[1;32m---> 29\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mget_text(separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\prach\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    318\u001b[0m ):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "chatgpt_data = read_json_file(HN_COMMENTS_DATASET_FILENAME)\n",
    "\n",
    "comments_rows, comments_github_urls = process_stories(chatgpt_data)\n",
    "\n",
    "fieldnames = [\n",
    "    'discussion_id',\n",
    "    'title',\n",
    "    'url',\n",
    "    'date',\n",
    "    'post_id',\n",
    "    'post_text',]\n",
    "\n",
    "write_csv('chatgpt_comments.csv', comments_rows, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
