{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KIDS = 100 # per story\n",
    "\n",
    "CHATGPT_PROGRESS_FILENAME = 'chatgpt_progress.json'\n",
    "CHATGPT_COMMENTS_PROGRESS_FILENAME = 'chatgpt_comments_progress.json'\n",
    "\n",
    "KEYWORDS_FILENAME = '../ai_keywords.txt'\n",
    "\n",
    "# chatgpt_gh_filename = 'github_links_chatgpt.json'\n",
    "\n",
    "# Define the base URL for the Hacker News API\n",
    "BASE_URL = 'https://hacker-news.firebaseio.com/v0'\n",
    "\n",
    "# CHATGPT_RELEASE_ID = 33804874\n",
    "# START_ID = 31300000 # may 8th 2022\n",
    "# END_ID = 40300000 # may 9th 2024\n",
    "\n",
    "DEPTH = 2 # comments depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "async def get_top_story_ids(session):\n",
    "    async with session.get(f'{BASE_URL}/topstories.json') as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def get_item(session, item_id):\n",
    "    async with session.get(f'{BASE_URL}/item/{item_id}.json') as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def get_kids_text(session, item, depth=DEPTH):\n",
    "    # Base condition for recursion\n",
    "    if 'kids' not in item or depth <= 0:\n",
    "        return []\n",
    "\n",
    "    kids_texts = []\n",
    "    tasks = []\n",
    "    for kid_id in item['kids'][:NUM_KIDS]:\n",
    "        tasks.append(get_item(session, kid_id))\n",
    "\n",
    "    # keep getting kids recursively depending on DEPTH\n",
    "    kids = await asyncio.gather(*tasks)\n",
    "    for kid in kids:\n",
    "        if kid and 'text' in kid:\n",
    "            kids_texts.append(kid['text'])\n",
    "            kids_texts.extend(await get_kids_text(session, kid, depth - 1))\n",
    "    return kids_texts\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'processed_story_max_id': -1, 'stories': []}\n",
    "\n",
    "def save_progress(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "async def retrieve_stories_comments(stories_source_filename, comments_dest_filename):\n",
    "    stories_dest = load_progress(comments_dest_filename)\n",
    "    progress_story_max_id = stories_dest['processed_story_max_id'] # inclusive - the last processed story id from the file\n",
    "    stories_comments_dest = stories_dest['stories']\n",
    "    curr_num_stories = len(stories_comments_dest)\n",
    "\n",
    "    stories_source = load_progress(stories_source_filename)\n",
    "    total_num_stories = len(stories_source['stories'])\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        pbar = tqdm(total=total_num_stories, desc=\"Fetching comments from stories\", )\n",
    "        pbar.update(curr_num_stories)\n",
    "\n",
    "        # go through each stories from the source\n",
    "        for story in stories_source['stories']:\n",
    "            try:\n",
    "                # if the story has already been searched, continue\n",
    "                if story['id'] <= progress_story_max_id:\n",
    "                    continue\n",
    "\n",
    "                # go through each comments and fetch the comment's data.\n",
    "                story['kids_text'] = await get_kids_text(session, story, depth=DEPTH)\n",
    "                stories_comments_dest.append(story)\n",
    "\n",
    "                # update progress, save to file\n",
    "                pbar.update(1)\n",
    "                progress_story_max_id = story['id']\n",
    "                save_progress({'processed_story_max_id': progress_story_max_id,\n",
    "                            'stories': stories_comments_dest}, comments_dest_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing story {story['id']}: {e}\")\n",
    "                # Save progress before exiting due to error\n",
    "                save_progress({'processed_story_max_id': progress_story_max_id,\n",
    "                            'stories': stories_comments_dest}, comments_dest_filename)\n",
    "        pbar.close()\n",
    "    return stories_comments_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f4a5fd5db34010b97a8102446165dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching comments from stories:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the original JSON file\n",
    "    with open(CHATGPT_PROGRESS_FILENAME, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Process the stories and fetch comments\n",
    "    processed_data = await retrieve_stories_comments(CHATGPT_PROGRESS_FILENAME, CHATGPT_COMMENTS_PROGRESS_FILENAME)\n",
    "\n",
    "    print('Done')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stories: 399\n",
      "Total number of comments: 14837\n"
     ]
    }
   ],
   "source": [
    "def count_comments(data):\n",
    "    return len(data), sum(len(story['kids_text']) for story in data)\n",
    "\n",
    "num_stories, num_comments = count_comments(processed_data)\n",
    "print(f\"Total number of stories: {num_stories}\")\n",
    "print(f\"Total number of comments: {num_comments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import html\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_github_urls(text):\n",
    "    github_url_pattern = r\"https://github\\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+(?:/(?:issues|pull)/\\d+)?\"\n",
    "    return list(set(re.findall(github_url_pattern, text)))\n",
    "\n",
    "def is_github_repo_or_issue(url):\n",
    "    if not url.startswith(\"https://github.com/\"):\n",
    "        return \"Not GitHub\"\n",
    "    if \"/issues/\" in url:\n",
    "        return \"GitHub Issue\"\n",
    "    if \"/pull/\" in url:\n",
    "        return \"GitHub PR\"\n",
    "    return \"GitHub Repo\"\n",
    "\n",
    "def unix_to_datetime(unix_timestamp):\n",
    "    return datetime.fromtimestamp(unix_timestamp)\n",
    "\n",
    "def clean_html(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def process_stories(discussions):\n",
    "    rows = []\n",
    "    github_urls = set()\n",
    "\n",
    "    for discussion in discussions['stories']:\n",
    "        discussion_id = discussion.get('id')\n",
    "        title = discussion.get('title', '')\n",
    "        url = discussion.get('url', '')\n",
    "        date = (discussion.get('time', 0))\n",
    "        # text = post.get('text', '')\n",
    "\n",
    "        github_urls.update(extract_github_urls(title))\n",
    "        github_urls.update(extract_github_urls(url))\n",
    "\n",
    "        if 'kids_text' in discussion:\n",
    "            for i, kid_text in enumerate(discussion['kids_text']):\n",
    "                cleaned_text = clean_html(kid_text)\n",
    "                print(i)\n",
    "                rows.append({\n",
    "                    'discussion_id': discussion_id,\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'date': date,\n",
    "                    'post_id': str(discussion_id)+'_'+str(i),\n",
    "                    'post_text': cleaned_text,\n",
    "                    # 'is_github_link': is_github_repo_or_issue(url),\n",
    "                })\n",
    "        else:\n",
    "            rows.append({\n",
    "                'discussion_id': discussion_id,\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'date': date,\n",
    "                'post_id': '',\n",
    "                'post_text': '',\n",
    "                # 'is_github_link': is_github_repo_or_issue(url),\n",
    "            })\n",
    "\n",
    "    return rows, github_urls\n",
    "\n",
    "def write_csv(filename, data, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_data = read_json_file(CHATGPT_COMMENTS_PROGRESS_FILENAME)\n",
    "\n",
    "comments_rows, comments_github_urls = process_stories(chatgpt_data)\n",
    "\n",
    "fieldnames = [\n",
    "    'discussion_id',\n",
    "    'title',\n",
    "    'url',\n",
    "    'date',\n",
    "    'post_id',\n",
    "    'post_text',]\n",
    "\n",
    "write_csv('chatgpt_comments.csv', comments_rows, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
