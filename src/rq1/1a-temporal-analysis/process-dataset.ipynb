{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1h2XGSbh_-czM5eM1DMZcLQmT-5vG8Ld3","authorship_tag":"ABX9TyMHsBufkDqWqcFHKmXj1k5C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# process comments dataset"],"metadata":{"id":"HCQyNjxW3deJ"}},{"cell_type":"markdown","source":["## RQ2: remove stories before chatgpt release"],"metadata":{"id":"oCyDDwzeFL_M"}},{"cell_type":"code","source":["input_filename = '/content/drive/MyDrive/datasets/data/hn_comments_dataset_final.csv' # Replace with your input CSV file\n","output_filename = '/content/drive/MyDrive/datasets/data/rq2/hn_comments_dataset_final_after_chatgpt_release.csv' # Replace with your desired output CSV file\n","input_filename = '/content/drive/MyDrive/datasets/muict-naist-senior/rq1/rq1_comments_all.csv'"],"metadata":{"id":"MzkMcKfysQ5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVTJ1tNhoH4o"},"outputs":[],"source":["import csv\n","from datetime import datetime\n","\n","def filter_comments(input_file, output_file, date_threshold):\n","    try:\n","        with open(input_file, 'r', encoding='utf-8') as infile, \\\n","                open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n","\n","            reader = csv.DictReader(infile)\n","            fieldnames = reader.fieldnames\n","            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n","            writer.writeheader()\n","\n","            for row in reader:\n","                try:\n","                    comment_date_str = row['discussion_date']\n","                    comment_date = datetime.strptime(comment_date_str, '%Y-%m-%d %H:%M:%S')\n","\n","                    if comment_date >= date_threshold:\n","                        writer.writerow(row)\n","                except ValueError:\n","                    print(f\"Skipping row due to invalid date format: {row}\")\n","        print(f\"Filtered comments written to {output_file}\")\n","    except FileNotFoundError:\n","        print(f\"Error: Input file '{input_file}' not found.\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n"]},{"cell_type":"code","source":["threshold_date = datetime(2022, 12, 1) # After ChatGPT release date\n","\n","filter_comments(input_filename, output_filename, threshold_date)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRXl5DyjsOBW","executionInfo":{"status":"ok","timestamp":1738047705825,"user_tz":-540,"elapsed":4281,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"e3a2e36c-b226-473f-d968-f75a2a776ad6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered comments written to /content/drive/MyDrive/datasets/data/rq2/hn_comments_dataset_final_after_chatgpt_release.csv\n"]}]},{"cell_type":"markdown","source":["# process story dataset"],"metadata":{"id":"wMSbL1w86TXH"}},{"cell_type":"markdown","source":["## RQ1 and RQ3: remove non-github repository rows"],"metadata":{"id":"rslJctJ8FJl-"}},{"cell_type":"code","source":["import csv\n","import re\n","from urllib.parse import urlparse, unquote"],"metadata":{"id":"cqg0P_50CHUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_github_repo_url(url):\n","    \"\"\"\n","    Check if a URL is a GitHub repository root URL.\n","    Returns True for URLs like 'https://github.com/username/repository'\n","    Returns False for URLs pointing to specific files, commits, pulls, gists, etc.\n","    \"\"\"\n","    try:\n","        # Parse the URL\n","        parsed = urlparse(unquote(url))\n","\n","        # Check if it's a GitHub URL\n","        if parsed.netloc != 'github.com':\n","            return False\n","\n","        # Split the path into components\n","        parts = [p for p in parsed.path.split('/') if p]\n","\n","        # A valid repo URL should have exactly 2 parts (username/repository)\n","        if len(parts) != 2:\n","            return False\n","\n","        # Check for specific patterns that indicate non-repository URLs\n","        non_repo_patterns = [\n","            r'/blob/',\n","            r'/tree/',\n","            r'/commit/',\n","            r'/pull/',\n","            r'/issues/',\n","            r'/releases/',\n","            r'/actions/',\n","            r'/wiki/',\n","            r'/settings/',\n","            r'/branches/'\n","        ]\n","\n","        return not any(pattern in url for pattern in non_repo_patterns)\n","\n","    except Exception:\n","        return False\n","\n","def filter_github_urls(input_file, output_file, url_column):\n","    \"\"\"\n","    Filter CSV file to keep only rows with valid GitHub repository URLs.\n","\n","    Args:\n","        input_file (str): Path to input CSV file\n","        output_file (str): Path to output CSV file\n","        url_column (str): Name of the column containing URLs\n","    \"\"\"\n","    with open(input_file, 'r', encoding='utf-8') as infile:\n","        reader = csv.DictReader(infile)\n","\n","        # Verify URL column exists\n","        if url_column not in reader.fieldnames:\n","            raise ValueError(f\"Column '{url_column}' not found in CSV file\")\n","\n","        with open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n","            writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n","            writer.writeheader()\n","\n","            for row in reader:\n","                url = row[url_column].strip()\n","                if is_github_repo_url(url):\n","                    writer.writerow(row)\n"],"metadata":{"id":"_y3weJYy6Waa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_filename = '/content/drive/MyDrive/datasets/data/hn_stories_dataset_gh_final.csv'\n","output_filename = '/content/drive/MyDrive/datasets/data/hn_stories_dataset_gh_final_v2.csv'\n","url_column = \"url\"  # Change this to match your CSV column name\n","\n","filter_github_urls(input_filename, output_filename, url_column)"],"metadata":{"id":"OmcMka2XCIiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6UmnrQaXC1RT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# brief stats"],"metadata":{"id":"dOQkSejbF-GA"}},{"cell_type":"markdown","source":["## comments"],"metadata":{"id":"mMyCHscNF_ew"}},{"cell_type":"code","source":["# prompt: import csv and count the number of entries (excluding headers). just in case it's useful, comments are defined by comment_id column\n","\n","import csv\n","\n","def count_entries(filename):\n","    try:\n","        with open(filename, 'r', encoding='utf-8') as csvfile:\n","            reader = csv.reader(csvfile)\n","            next(reader)  # Skip the header row\n","            entry_count = sum(1 for row in reader)\n","            return entry_count\n","    except FileNotFoundError:\n","        print(f\"Error: File '{filename}' not found.\")\n","        return 0\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return 0\n","\n","# Example usage (replace with your actual filename):\n","filename = '/content/drive/MyDrive/datasets/data/hn_comments_dataset_final_v2.csv'\n","entry_count = count_entries(filename)\n","print(f\"{entry_count} comments\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrpnNXbIF-79","executionInfo":{"status":"ok","timestamp":1738054190794,"user_tz":-540,"elapsed":297,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"d9801e55-d33e-47d8-9671-2f50202f38e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2963 comments\n"]}]},{"cell_type":"markdown","source":["## stories"],"metadata":{"id":"BmNHbXboGCEZ"}},{"cell_type":"code","source":["# prompt: import csv and count the number of entries (excluding headers). just in case it's useful, stories are defined by discussion_id column\n","\n","def count_stories(filename):\n","    try:\n","        with open(filename, 'r', encoding='utf-8') as csvfile:\n","            reader = csv.DictReader(csvfile)\n","            stories = set()\n","            for row in reader:\n","                stories.add(row['discussion_id'])\n","            return len(stories)\n","    except FileNotFoundError:\n","        print(f\"Error: File '{filename}' not found.\")\n","        return 0\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return 0\n","\n","# Example usage (replace with your actual filename):\n","filename = '/content/drive/MyDrive/datasets/data/hn_stories_dataset_gh_final_v2.csv'\n","story_count = count_stories(filename)\n","print(f\"{story_count} stories\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6N5McNxGDUK","executionInfo":{"status":"ok","timestamp":1738054225318,"user_tz":-540,"elapsed":241,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"baddec71-0d1b-4210-a1ae-56bf6b7047e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300 stories\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3d9MNeElGssn"},"execution_count":null,"outputs":[]}]}