{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HackerNews repo data...\n",
      "Loading non-HackerNews repo data...\n",
      "Combining datasets...\n",
      "\n",
      "Analyzing causal effect on PRs...\n",
      "Checking parallel trends assumption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DiD analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated causal effect on PRs: -3.346403150464679\n",
      "Confidence interval not available.\n",
      "Visualizing treatment effect...\n",
      "Running placebo tests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real effect estimate: -25.297756621863037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 451\u001b[39m\n\u001b[32m    447\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_repos_data, results\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m all_repos_data, results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 407\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# Run placebo tests\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning placebo tests...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m real_estimate, placebo_estimates, p_value = \u001b[43mplacebo_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlacebo test p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36mplacebo_test\u001b[39m\u001b[34m(df, outcome_var, num_placebos)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Run DiD analysis\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     placebo_estimate = \u001b[43mdid_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplacebo_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcome_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m     placebo_estimates.append(placebo_estimate.value)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mdid_analysis\u001b[39m\u001b[34m(df, outcome_var)\u001b[39m\n\u001b[32m    159\u001b[39m identified_estimand = model.identify_effect(proceed_when_unidentifiable=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# Estimate causal effect using DiD estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m estimate = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimate_effect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43midentified_estimand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackdoor.econml.dml.DML\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_value\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtreatment_value\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_units\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43matt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Average Treatment Effect on the Treated\u001b[39;49;00m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_params\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_y\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLassoCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_t\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLassoCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_final\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRandomForestRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimate\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\dowhy\\causal_model.py:361\u001b[39m, in \u001b[36mCausalModel.estimate_effect\u001b[39m\u001b[34m(self, identified_estimand, method_name, control_value, treatment_value, test_significance, evaluate_effect_strength, confidence_intervals, target_units, effect_modifiers, fit_estimator, method_params)\u001b[39m\n\u001b[32m    350\u001b[39m         causal_estimator = causal_estimator_class(\n\u001b[32m    351\u001b[39m             identified_estimand,\n\u001b[32m    352\u001b[39m             test_significance=test_significance,\n\u001b[32m   (...)\u001b[39m\u001b[32m    356\u001b[39m             **extra_args,\n\u001b[32m    357\u001b[39m         )\n\u001b[32m    359\u001b[39m         \u001b[38;5;28mself\u001b[39m._estimator_cache[method_name] = causal_estimator\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimate_effect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_treatment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_outcome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43midentifier_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrol_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtreatment_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43meffect_modifiers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\dowhy\\causal_estimator.py:752\u001b[39m, in \u001b[36mestimate_effect\u001b[39m\u001b[34m(data, treatment, outcome, identifier_name, estimator, control_value, treatment_value, target_units, effect_modifiers, fit_estimator, method_params)\u001b[39m\n\u001b[32m    747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CausalEstimate(\n\u001b[32m    748\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, control_value=control_value, treatment_value=treatment_value\n\u001b[32m    749\u001b[39m     )\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fit_estimator:\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43meffect_modifier_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffect_modifiers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmethod_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_params\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_params\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmethod_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m estimate = estimator.estimate_effect(\n\u001b[32m    759\u001b[39m     data,\n\u001b[32m    760\u001b[39m     treatment_value=treatment_value,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     confidence_intervals=estimator._confidence_intervals,\n\u001b[32m    764\u001b[39m )\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator._significance_test:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\dowhy\\causal_estimators\\econml.py:190\u001b[39m, in \u001b[36mEconml.fit\u001b[39m\u001b[34m(self, data, effect_modifier_names, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m estimator_named_args = estimator_argspec.args + estimator_argspec.kwonlyargs\n\u001b[32m    187\u001b[39m estimator_data_args = {\n\u001b[32m    188\u001b[39m     arg: named_data_args[arg] \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m named_data_args.keys() \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m estimator_named_args\n\u001b[32m    189\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mestimator_data_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\dml\\dml.py:589\u001b[39m, in \u001b[36mDML.fit\u001b[39m\u001b[34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, T, *, X=\u001b[38;5;28;01mNone\u001b[39;00m, W=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, freq_weight=\u001b[38;5;28;01mNone\u001b[39;00m, sample_var=\u001b[38;5;28;01mNone\u001b[39;00m, groups=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    551\u001b[39m         cache_values=\u001b[38;5;28;01mFalse\u001b[39;00m, inference=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    552\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[33;03m    Estimate the counterfactual model from data, i.e. estimates functions τ(·,·,·), ∂τ(·,·).\u001b[39;00m\n\u001b[32m    554\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    587\u001b[39m \u001b[33;03m    self\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m                       \u001b[49m\u001b[43msample_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mcache_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m                       \u001b[49m\u001b[43minference\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\dml\\_rlearner.py:415\u001b[39m, in \u001b[36m_RLearner.fit\u001b[39m\u001b[34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03mEstimate the counterfactual model from data, i.e. estimates function :math:`\\\\theta(\\\\cdot)`.\u001b[39;00m\n\u001b[32m    380\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \u001b[33;03mself: _RLearner instance\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_var\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mcache_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m                   \u001b[49m\u001b[43minference\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\_cate_estimator.py:131\u001b[39m, in \u001b[36mBaseCateEstimator._wrap_fit.<locals>.call\u001b[39m\u001b[34m(self, Y, T, inference, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     inference.prefit(\u001b[38;5;28mself\u001b[39m, Y, T, *args, **kwargs)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# call the wrapped fit method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m._postfit(Y, T, *args, **kwargs)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# NOTE: we call inference fit *after* calling the main fit method\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\_ortho_learner.py:822\u001b[39m, in \u001b[36m_OrthoLearner.fit\u001b[39m\u001b[34m(self, Y, T, X, W, Z, sample_weight, freq_weight, sample_var, groups, cache_values, inference, only_final, check_input)\u001b[39m\n\u001b[32m    820\u001b[39m     nuisances, fitted_models, new_inds, scores = ray.get(\u001b[38;5;28mself\u001b[39m.nuisances_ref[idx])\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     nuisances, fitted_models, new_inds, scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_nuisances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_nuisances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    824\u001b[39m all_nuisances.append(nuisances)\n\u001b[32m    825\u001b[39m \u001b[38;5;28mself\u001b[39m._models_nuisance.append(fitted_models)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\_ortho_learner.py:972\u001b[39m, in \u001b[36m_OrthoLearner._fit_nuisances\u001b[39m\u001b[34m(self, Y, T, X, W, Z, sample_weight, groups)\u001b[39m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    970\u001b[39m         folds = splitter.split(to_split, strata)\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m nuisances, fitted_models, fitted_inds, scores = \u001b[43m_crossfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ortho_learner_model_nuisance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_ray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mray_remote_func_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m                                                          \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m nuisances, fitted_models, fitted_inds, scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\_ortho_learner.py:280\u001b[39m, in \u001b[36m_crossfit\u001b[39m\u001b[34m(models, folds, use_ray, ray_remote_fun_option, *args, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m     nuisance_temp, model_out, score_temp = ray.get(fold_refs[idx])\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     nuisance_temp, model_out, score_temp = \u001b[43m_fit_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mcalculate_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulated_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx == \u001b[32m0\u001b[39m:\n\u001b[32m    284\u001b[39m     nuisances = \u001b[38;5;28mtuple\u001b[39m([np.full((n,) + nuis.shape[\u001b[32m1\u001b[39m:], np.nan)\n\u001b[32m    285\u001b[39m                       \u001b[38;5;28;01mfor\u001b[39;00m nuis \u001b[38;5;129;01min\u001b[39;00m nuisance_temp])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\_ortho_learner.py:99\u001b[39m, in \u001b[36m_fit_fold\u001b[39m\u001b[34m(model, train_idxs, test_idxs, calculate_scores, args, kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m kwargs_train = {key: var[train_idxs] \u001b[38;5;28;01mfor\u001b[39;00m key, var \u001b[38;5;129;01min\u001b[39;00m kwargs.items()}\n\u001b[32m     97\u001b[39m kwargs_test = {key: var[test_idxs] \u001b[38;5;28;01mfor\u001b[39;00m key, var \u001b[38;5;129;01min\u001b[39;00m kwargs.items()}\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m nuisance_temp = model.predict(*args_test, **kwargs_test)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nuisance_temp, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\dml\\_rlearner.py:55\u001b[39m, in \u001b[36m_ModelNuisance.train\u001b[39m\u001b[34m(self, is_selecting, folds, Y, T, X, W, Z, sample_weight, groups)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m Z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mCannot accept instrument!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m._model_t.train(is_selecting, folds, X, W, T, **\n\u001b[32m     54\u001b[39m                     filter_none_kwargs(sample_weight=sample_weight, groups=groups))\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_selecting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfilter_none_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\dml\\dml.py:95\u001b[39m, in \u001b[36m_FirstStageSelector.train\u001b[39m\u001b[34m(self, is_selecting, folds, X, W, Target, sample_weight, groups)\u001b[39m\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mProvided crossfit folds contain training splits that \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     92\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mdon\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt contain all treatments\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m     Target = inverse_onehot(Target)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_selecting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                  \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_none_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\econml\\sklearn_extensions\\model_selection.py:562\u001b[39m, in \u001b[36mSklearnCVSelector.train\u001b[39m\u001b[34m(self, is_selecting, folds, groups, *args, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28mself\u001b[39m._best_model, \u001b[38;5;28mself\u001b[39m._best_score = \u001b[38;5;28mself\u001b[39m._convert_model(\u001b[38;5;28mself\u001b[39m.searcher, args, kwargs)\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1041\u001b[39m, in \u001b[36mElasticNet.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1032\u001b[39m     \u001b[38;5;66;03m# Note: Alternatively, we could also have rescaled alpha instead\u001b[39;00m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# of sample_weight:\u001b[39;00m\n\u001b[32m   1034\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# X and y will be rescaled if sample_weight is not None, order='F'\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# ensures that the returned X and y are still F-contiguous.\u001b[39;00m\n\u001b[32m   1040\u001b[39m should_copy = \u001b[38;5;28mself\u001b[39m.copy_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X_copied\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m X, y, X_offset, y_offset, X_scale, precompute, Xy = \u001b[43m_pre_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshould_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# coordinate descent needs F-ordered arrays and _pre_fit might have\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# called _rescale_data\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_input \u001b[38;5;129;01mor\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:790\u001b[39m, in \u001b[36m_pre_fit\u001b[39m\u001b[34m(X, y, Xy, precompute, fit_intercept, copy, check_input, sample_weight)\u001b[39m\n\u001b[32m    780\u001b[39m     X, y, X_offset, y_offset, X_scale = _preprocess_data(\n\u001b[32m    781\u001b[39m         X,\n\u001b[32m    782\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m         sample_weight=sample_weight,\n\u001b[32m    787\u001b[39m     )\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    789\u001b[39m     \u001b[38;5;66;03m# copy was done in fit if necessary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     X, y, X_offset, y_offset, X_scale = \u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m     \u001b[38;5;66;03m# Rescale only in dense case. Sparse cd solver directly deals with\u001b[39;00m\n\u001b[32m    799\u001b[39m     \u001b[38;5;66;03m# sample_weight.\u001b[39;00m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    801\u001b[39m         \u001b[38;5;66;03m# This triggers copies anyway.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:193\u001b[39m, in \u001b[36m_preprocess_data\u001b[39m\u001b[34m(X, y, fit_intercept, copy, copy_y, sample_weight, check_input)\u001b[39m\n\u001b[32m    191\u001b[39m     X_offset, X_var = mean_variance_axis(X, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     X_offset = \u001b[43m_average\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     X_offset = xp.astype(X_offset, X.dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    196\u001b[39m     X -= X_offset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:635\u001b[39m, in \u001b[36m_average\u001b[39m\u001b[34m(a, axis, weights, normalize, xp)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    637\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(numpy.dot(a, weights))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\numpy\\lib\\function_base.py:520\u001b[39m, in \u001b[36maverage\u001b[39m\u001b[34m(a, axis, weights, returned, keepdims)\u001b[39m\n\u001b[32m    517\u001b[39m     keepdims_kw = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims}\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     avg = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkeepdims_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     avg_as_array = np.asanyarray(avg)\n\u001b[32m    522\u001b[39m     scl = avg_as_array.dtype.type(a.size/avg_as_array.size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prach\\miniconda3\\envs\\naist-2025\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[39m, in \u001b[36m_mean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m    115\u001b[39m         dtype = mu.dtype(\u001b[33m'\u001b[39m\u001b[33mf4\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    116\u001b[39m         is_float16_result = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu.ndarray):\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dowhy import CausalModel\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "# Load and prepare data\n",
    "def load_and_prepare_data(metrics_files, hn_submission_file=None):\n",
    "    \"\"\"\n",
    "    Load and prepare data from multiple CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    metrics_files: Dictionary mapping metric names to file paths\n",
    "    hn_submission_file: Path to HackerNews submission data (None for non-HN repos)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame ready for causal inference\n",
    "    \"\"\"\n",
    "    # Load metrics data\n",
    "    metrics_data = {}\n",
    "    for metric_name, file_path in metrics_files.items():\n",
    "        metrics_data[metric_name] = pd.read_csv(file_path)\n",
    "\n",
    "    # Start with one metric as base and get the repo list\n",
    "    base_metric = list(metrics_data.keys())[0]\n",
    "    repo_list = metrics_data[base_metric]['repo_url'].tolist()\n",
    "\n",
    "    # Convert wide format to long format for each metric\n",
    "    all_data = []\n",
    "\n",
    "    for metric_name, df in metrics_data.items():\n",
    "        # Melt the dataframe to convert from wide to long format\n",
    "        id_vars = ['repo_url']\n",
    "        value_vars = [col for col in df.columns if col != 'repo_url']\n",
    "\n",
    "        melted_df = pd.melt(df,\n",
    "                            id_vars=id_vars,\n",
    "                            value_vars=value_vars,\n",
    "                            var_name='month',\n",
    "                            value_name=metric_name)\n",
    "\n",
    "        # Filter out data points where the repo didn't exist yet (-1.0 values)\n",
    "        melted_df = melted_df[(melted_df[metric_name] != -1.0) & (melted_df[metric_name] != -2.0)]\n",
    "\n",
    "        if len(all_data) == 0:\n",
    "            all_data = melted_df\n",
    "        else:\n",
    "            # Merge with existing data\n",
    "            all_data = pd.merge(all_data, melted_df, on=['repo_url', 'month'])\n",
    "\n",
    "    # Convert month string to datetime\n",
    "    all_data['date'] = pd.to_datetime(all_data['month'], format='%Y-%m')\n",
    "    all_data.drop('month', axis=1, inplace=True)\n",
    "\n",
    "    # Add HN submission information if available\n",
    "    if hn_submission_file is not None:\n",
    "        # Load HN submission data\n",
    "        hn_data = pd.read_csv(hn_submission_file)\n",
    "\n",
    "        # Convert Unix timestamp to datetime\n",
    "        hn_data['submission_date'] = pd.to_datetime(hn_data['date'], unit='s')\n",
    "        hn_data = hn_data[['url', 'submission_date']]\n",
    "        hn_data.rename(columns={'url': 'repo_url'}, inplace=True)\n",
    "\n",
    "        # Merge with metrics data\n",
    "        all_data = pd.merge(all_data, hn_data, on='repo_url', how='left')\n",
    "\n",
    "        # Add treatment indicator\n",
    "        all_data['hn_submitted'] = all_data['submission_date'].notna().astype(int)\n",
    "\n",
    "        # Create post-treatment indicator\n",
    "        all_data['post_treatment'] = 0\n",
    "        mask = (all_data['submission_date'].notna()) & (all_data['date'] >= all_data['submission_date'])\n",
    "        all_data.loc[mask, 'post_treatment'] = 1\n",
    "\n",
    "        # Create treatment variable\n",
    "        all_data['treatment'] = all_data['hn_submitted'] * all_data['post_treatment']\n",
    "    else:\n",
    "        # For non-HN repos\n",
    "        all_data['hn_submitted'] = 0\n",
    "        all_data['submission_date'] = np.nan\n",
    "        all_data['post_treatment'] = 0\n",
    "        all_data['treatment'] = 0\n",
    "\n",
    "    # Create time period column (months since start)\n",
    "    min_date = all_data['date'].min()\n",
    "    all_data['time_period'] = ((all_data['date'].dt.year - min_date.year) * 12 +\n",
    "                              (all_data['date'].dt.month - min_date.month))\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Combine HN and non-HN repo data\n",
    "def combine_datasets(hn_repos_data, non_hn_repos_data):\n",
    "    \"\"\"\n",
    "    Combine datasets from HN-submitted and non-HN-submitted repos.\n",
    "\n",
    "    Parameters:\n",
    "    hn_repos_data: DataFrame with data for repos submitted to HackerNews\n",
    "    non_hn_repos_data: DataFrame with data for repos not submitted to HackerNews\n",
    "\n",
    "    Returns:\n",
    "    Combined DataFrame\n",
    "    \"\"\"\n",
    "    # Make sure the columns match\n",
    "    common_cols = set(hn_repos_data.columns).intersection(set(non_hn_repos_data.columns))\n",
    "\n",
    "    # Add any missing columns to non_hn_repos_data\n",
    "    for col in hn_repos_data.columns:\n",
    "        if col not in common_cols:\n",
    "            if col in ['submission_date']:\n",
    "                non_hn_repos_data[col] = np.nan\n",
    "            elif col in ['hn_submitted', 'post_treatment', 'treatment']:\n",
    "                non_hn_repos_data[col] = 0\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_data = pd.concat([hn_repos_data, non_hn_repos_data], ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Perform DiD analysis using DoWhy\n",
    "def did_analysis(df, outcome_var='PRs'):\n",
    "    \"\"\"\n",
    "    Perform Difference-in-Differences analysis using DoWhy.\n",
    "\n",
    "    Parameters:\n",
    "    df: Prepared DataFrame\n",
    "    outcome_var: Outcome variable to measure (default: PRs)\n",
    "\n",
    "    Returns:\n",
    "    Causal effect estimate\n",
    "    \"\"\"\n",
    "    # Define causal graph\n",
    "    graph = \"\"\"\n",
    "    digraph {\n",
    "        time_period -> %s;\n",
    "        repo_url -> %s;\n",
    "        treatment -> %s;\n",
    "        repo_url -> treatment;\n",
    "    }\n",
    "    \"\"\" % (outcome_var, outcome_var, outcome_var)\n",
    "\n",
    "    # Specify variables\n",
    "    treatment_var = \"treatment\"\n",
    "    common_causes = [\"repo_url\", \"time_period\"]\n",
    "\n",
    "    # Create causal model\n",
    "    model = CausalModel(\n",
    "        data=df,\n",
    "        treatment=treatment_var,\n",
    "        outcome=outcome_var,\n",
    "        graph=graph,\n",
    "        common_causes=common_causes\n",
    "    )\n",
    "\n",
    "    # Identify causal effect\n",
    "    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "\n",
    "    # Estimate causal effect using DiD estimator\n",
    "    estimate = model.estimate_effect(\n",
    "        identified_estimand,\n",
    "        method_name=\"backdoor.econml.dml.DML\",\n",
    "        control_value=0,\n",
    "        treatment_value=1,\n",
    "        target_units=\"att\",  # Average Treatment Effect on the Treated\n",
    "        method_params={\n",
    "            \"init_params\": {\n",
    "                \"model_y\": LassoCV(),\n",
    "                \"model_t\": LassoCV(),\n",
    "                \"model_final\": RandomForestRegressor(n_estimators=100, min_samples_leaf=10)\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return estimate\n",
    "\n",
    "# Heterogeneous treatment effects\n",
    "def heterogeneous_effects(df, outcome_var='PRs'):\n",
    "    \"\"\"\n",
    "    Analyze heterogeneous treatment effects using CausalForest.\n",
    "\n",
    "    Parameters:\n",
    "    df: Prepared DataFrame\n",
    "    outcome_var: Outcome variable to measure (default: PRs)\n",
    "\n",
    "    Returns:\n",
    "    CausalForest model and DataFrame with CATE estimates\n",
    "    \"\"\"\n",
    "    # Select numeric features (excluding the outcome and treatment variables)\n",
    "    feature_cols = [col for col in df.columns if df[col].dtype in [np.int64, np.float64]]\n",
    "    feature_cols = [col for col in feature_cols if col not in\n",
    "                    [outcome_var, 'treatment', 'hn_submitted', 'post_treatment']]\n",
    "\n",
    "    # Prepare features\n",
    "    features = df[feature_cols]\n",
    "    treatment = df['treatment']\n",
    "    outcome = df[outcome_var]\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    mask = ~(features.isna().any(axis=1) | treatment.isna() | outcome.isna())\n",
    "    features = features[mask]\n",
    "    treatment = treatment[mask]\n",
    "    outcome = outcome[mask]\n",
    "\n",
    "    # Fit causal forest model\n",
    "    cf_model = CausalForestDML(\n",
    "        model_y=None,  # Will use default models\n",
    "        model_t=None,\n",
    "        n_estimators=100,\n",
    "        min_samples_leaf=10\n",
    "    )\n",
    "    cf_model.fit(features, treatment, outcome)\n",
    "\n",
    "    # Calculate conditional treatment effects\n",
    "    cate_estimates = cf_model.effect(features)\n",
    "\n",
    "    # Add CATE estimates to DataFrame\n",
    "    df_with_cate = df[mask].copy()\n",
    "    df_with_cate['cate'] = cate_estimates\n",
    "\n",
    "    return cf_model, df_with_cate\n",
    "\n",
    "# Validate results with placebo tests\n",
    "def placebo_test(df, outcome_var='PRs', num_placebos=50):\n",
    "    \"\"\"\n",
    "    Perform placebo tests by randomizing treatment assignment.\n",
    "\n",
    "    Parameters:\n",
    "    df: Prepared DataFrame\n",
    "    outcome_var: Outcome variable to measure (default: PRs)\n",
    "    num_placebos: Number of placebo tests to run\n",
    "\n",
    "    Returns:\n",
    "    Array of placebo estimates and p-value\n",
    "    \"\"\"\n",
    "    # Run actual analysis\n",
    "    real_estimate = did_analysis(df, outcome_var)\n",
    "    placebo_estimates = []\n",
    "\n",
    "    print(f\"Real effect estimate: {real_estimate.value}\")\n",
    "\n",
    "    for i in range(num_placebos):\n",
    "        # Create copy of DataFrame\n",
    "        placebo_df = df.copy()\n",
    "\n",
    "        # Randomize treatment assignment\n",
    "        placebo_df['treatment'] = np.random.permutation(placebo_df['treatment'].values)\n",
    "\n",
    "        # Run DiD analysis\n",
    "        try:\n",
    "            placebo_estimate = did_analysis(placebo_df, outcome_var)\n",
    "            placebo_estimates.append(placebo_estimate.value)\n",
    "        except Exception as e:\n",
    "            print(f\"Placebo test {i} failed: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate p-value\n",
    "    p_value = sum(abs(pe) >= abs(real_estimate.value) for pe in placebo_estimates) / len(placebo_estimates)\n",
    "\n",
    "    return real_estimate, placebo_estimates, p_value\n",
    "\n",
    "# Visualize parallel trends assumption\n",
    "def check_parallel_trends(df, outcome_var='PRs'):\n",
    "    \"\"\"\n",
    "    Check the parallel trends assumption by plotting pre-treatment trends.\n",
    "\n",
    "    Parameters:\n",
    "    df: Prepared DataFrame\n",
    "    outcome_var: Outcome variable to measure (default: PRs)\n",
    "    \"\"\"\n",
    "    # Get unique repos that were submitted to HN\n",
    "    hn_repos = df[df['hn_submitted'] == 1]['repo_url'].unique()\n",
    "\n",
    "    # Filter for pre-treatment periods for these repos\n",
    "    pre_treatment = df[(df['repo_url'].isin(hn_repos)) & (df['post_treatment'] == 0)]\n",
    "    control_group = df[df['hn_submitted'] == 0]\n",
    "\n",
    "    # Aggregate by time period\n",
    "    treated_pre = pre_treatment.groupby('time_period')[outcome_var].mean().reset_index()\n",
    "    control_all = control_group.groupby('time_period')[outcome_var].mean().reset_index()\n",
    "\n",
    "    # Get the earliest treatment period\n",
    "    min_treatment_period = df[df['post_treatment'] == 1]['time_period'].min()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(treated_pre['time_period'], treated_pre[outcome_var], 'b-',\n",
    "             label='Pre-treatment (HN Repos)')\n",
    "    plt.plot(control_all['time_period'], control_all[outcome_var], 'r-',\n",
    "             label='Control (Non-HN Repos)')\n",
    "    plt.axvline(x=min_treatment_period, color='green', linestyle='--',\n",
    "                label='First Treatment')\n",
    "    plt.xlabel('Time Period (Months since start)')\n",
    "    plt.ylabel(f'Average {outcome_var}')\n",
    "    plt.title(f'Parallel Trends Check: {outcome_var}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'parallel_trends_{outcome_var}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Visualize treatment effect\n",
    "def visualize_treatment_effect(df, outcome_var='PRs'):\n",
    "    \"\"\"\n",
    "    Visualize the treatment effect over time.\n",
    "\n",
    "    Parameters:\n",
    "    df: Prepared DataFrame\n",
    "    outcome_var: Outcome variable to measure (default: PRs)\n",
    "    \"\"\"\n",
    "    # Aggregate data by time period and treatment status\n",
    "    treatment_data = df[df['hn_submitted'] == 1].copy()\n",
    "    control_data = df[df['hn_submitted'] == 0].copy()\n",
    "\n",
    "    # For repos that were submitted to HN, mark pre and post treatment periods\n",
    "    treatment_pre = treatment_data[treatment_data['post_treatment'] == 0].groupby('time_period')[outcome_var].mean().reset_index()\n",
    "    treatment_post = treatment_data[treatment_data['post_treatment'] == 1].groupby('time_period')[outcome_var].mean().reset_index()\n",
    "    control_all = control_data.groupby('time_period')[outcome_var].mean().reset_index()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(treatment_pre['time_period'], treatment_pre[outcome_var], 'b-',\n",
    "             label='Pre-treatment (HN Repos)')\n",
    "    plt.plot(treatment_post['time_period'], treatment_post[outcome_var], 'g-',\n",
    "             label='Post-treatment (HN Repos)')\n",
    "    plt.plot(control_all['time_period'], control_all[outcome_var], 'r-',\n",
    "             label='Control (Non-HN Repos)')\n",
    "\n",
    "    # Add a vertical line at the first treatment\n",
    "    first_treatment = treatment_data[treatment_data['post_treatment'] == 1]['time_period'].min()\n",
    "    plt.axvline(x=first_treatment, color='black', linestyle='--',\n",
    "                label='First HN Submission')\n",
    "\n",
    "    plt.xlabel('Time Period (Months since start)')\n",
    "    plt.ylabel(f'Average {outcome_var}')\n",
    "    plt.title(f'Effect of HackerNews Submission on {outcome_var}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'treatment_effect_{outcome_var}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main function to run the analysis\n",
    "def main():\n",
    "    # File paths (replace with your actual file paths)\n",
    "    hn_metrics_files = {\n",
    "        'stars': 'hn_stars_metrics.csv',\n",
    "        'forks': 'hn_forks_metrics.csv',\n",
    "        'commits': 'hn_commits_metrics.csv',\n",
    "        'PRs': 'hn_prs_metrics.csv',\n",
    "        'contributors': 'hn_contributors_metrics.csv'\n",
    "    }\n",
    "\n",
    "    non_hn_metrics_files = {\n",
    "        'stars': 'non_hn_stars_metrics.csv',\n",
    "        'forks': 'non_hn_forks_metrics.csv',\n",
    "        'commits': 'non_hn_commits_metrics.csv',\n",
    "        'PRs': 'non_hn_prs_metrics.csv',\n",
    "        'contributors': 'non_hn_contributors_metrics.csv'\n",
    "    }\n",
    "\n",
    "    hn_submission_file = 'rq1_stories_github_valid_projs_only_292.csv'\n",
    "\n",
    "    # Load and prepare data\n",
    "    print(\"Loading HackerNews repo data...\")\n",
    "    hn_repos_data = load_and_prepare_data(hn_metrics_files, hn_submission_file)\n",
    "\n",
    "    print(\"Loading non-HackerNews repo data...\")\n",
    "    non_hn_repos_data = load_and_prepare_data(non_hn_metrics_files)\n",
    "\n",
    "    print(\"Combining datasets...\")\n",
    "    all_repos_data = combine_datasets(hn_repos_data, non_hn_repos_data)\n",
    "\n",
    "    # Drop rows with NaN values in the outcome variable\n",
    "    outcomes = ['PRs', 'stars', 'forks', 'commits', 'contributors']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for outcome_var in outcomes:\n",
    "        print(f\"\\nAnalyzing causal effect on {outcome_var}...\")\n",
    "\n",
    "        # Drop NaN values for this specific outcome\n",
    "        analysis_data = all_repos_data.dropna(subset=[outcome_var])\n",
    "\n",
    "        # Check parallel trends assumption\n",
    "        print(\"Checking parallel trends assumption...\")\n",
    "        check_parallel_trends(analysis_data, outcome_var)\n",
    "\n",
    "        # Run DiD analysis\n",
    "        print(\"Running DiD analysis...\")\n",
    "        estimate = did_analysis(analysis_data, outcome_var)\n",
    "\n",
    "        print(f\"Estimated causal effect on {outcome_var}: {estimate.value}\")\n",
    "        if hasattr(estimate, \"confidence_intervals\") and estimate.confidence_intervals is not None:\n",
    "            lower, upper = estimate.confidence_intervals[0]\n",
    "            print(f\"95% Confidence Interval: ({lower}, {upper})\")\n",
    "        else:\n",
    "            print(\"Confidence interval not available.\")\n",
    "\n",
    "\n",
    "        # Visualize treatment effect\n",
    "        print(\"Visualizing treatment effect...\")\n",
    "        visualize_treatment_effect(analysis_data, outcome_var)\n",
    "\n",
    "        # Run placebo tests\n",
    "        print(\"Running placebo tests...\")\n",
    "        real_estimate, placebo_estimates, p_value = placebo_test(analysis_data, outcome_var)\n",
    "\n",
    "        print(f\"Placebo test p-value: {p_value}\")\n",
    "\n",
    "        # Store results\n",
    "        results[outcome_var] = {\n",
    "            'estimate': estimate.value,\n",
    "            'stderr': estimate.stderr,\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n",
    "        # Optional: Analyze heterogeneous effects\n",
    "        print(\"Analyzing heterogeneous effects...\")\n",
    "        try:\n",
    "            cf_model, df_with_cate = heterogeneous_effects(analysis_data, outcome_var)\n",
    "\n",
    "            # Save the top and bottom repos by treatment effect\n",
    "            top_repos = df_with_cate.sort_values('cate', ascending=False).head(10)\n",
    "            bottom_repos = df_with_cate.sort_values('cate', ascending=True).head(10)\n",
    "\n",
    "            print(f\"Top 10 repos with highest treatment effect on {outcome_var}:\")\n",
    "            print(top_repos[['repo_url', 'cate']].drop_duplicates('repo_url').head())\n",
    "\n",
    "            print(f\"Bottom 10 repos with lowest treatment effect on {outcome_var}:\")\n",
    "            print(bottom_repos[['repo_url', 'cate']].drop_duplicates('repo_url').head())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Heterogeneous effects analysis failed: {str(e)}\")\n",
    "\n",
    "    # Summary of results\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for outcome, result in results.items():\n",
    "        print(f\"Outcome: {outcome}\")\n",
    "        print(f\"  Causal Effect: {result['estimate']:.4f} ± {result['stderr']:.4f}\")\n",
    "        print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "        if result['p_value'] < 0.05:\n",
    "            print(\"  Result is statistically significant at p < 0.05\")\n",
    "        else:\n",
    "            print(\"  Result is not statistically significant at p < 0.05\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return all_repos_data, results\n",
    "\n",
    "all_repos_data, results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naist-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
