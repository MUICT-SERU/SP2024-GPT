{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1THwkzX1z1y-OJixAD2tT7W_fCrvP4a6_","authorship_tag":"ABX9TyPXLwHuHAR9cHjK6thtgsW9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# v1 combine all output csv (naive)"],"metadata":{"id":"d7-0i8URDj9o"}},{"cell_type":"code","source":["import pandas as pd\n","import glob\n","\n","# List of metrics\n","metrics = [\"commits\", \"contributors\", \"forks\", \"pull_requests\", \"stars\"]\n","\n","# Loop through each metric\n","for metric in metrics:\n","    # Get all CSV files for the current metric\n","    csv_files = sorted(glob.glob(f\"*_{metric}_metrics.csv\"))\n","\n","    # Read and concatenate all CSV files for this metric\n","    df_list = [pd.read_csv(file, index_col=0) for file in csv_files]\n","    combined_df = pd.concat(df_list)\n","\n","    # Drop duplicates if any (keeping the first occurrence)\n","    combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n","\n","    # Save the combined CSV\n","    combined_df.to_csv(f\"{metric}_combined.csv\")\n","    print(f\"Saved {metric}_combined.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XA7hz5dNDjZE","executionInfo":{"status":"ok","timestamp":1740850240192,"user_tz":-420,"elapsed":678,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"5c50fe84-d807-400b-b15f-a1a88025c1e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved commits_combined.csv\n","Saved contributors_combined.csv\n","Saved forks_combined.csv\n","Saved pull_requests_combined.csv\n","Saved stars_combined.csv\n"]}]},{"cell_type":"markdown","source":["# v2 - csv metric merger (account for non-matching dates/repo)"],"metadata":{"id":"G0lJFRY79OpP"}},{"cell_type":"code","source":["# Simple CSV Merger for GitHub Metrics Data\n","\n","import pandas as pd\n","import os\n","import glob\n","import re\n","from datetime import datetime\n","\n","# Function to combine CSV files by metric type\n","def combine_metric_files(directory='.', output_dir='output'):\n","    \"\"\"\n","    Combines CSV files by metric type (commits, pull_requests, stars, contributors, forks)\n","    and saves them as separate files.\n","\n","    Args:\n","        directory (str): Directory containing the input CSV files\n","        output_dir (str): Directory to save the output files\n","    \"\"\"\n","    # Ensure output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Define metric types to look for\n","    metric_types = ['commits', 'pull_requests', 'stars', 'contributors', 'forks']\n","\n","    # Process each metric type\n","    for metric_type in metric_types:\n","        print(f\"Processing {metric_type} files...\")\n","\n","        # Find all CSV files for this metric type\n","        pattern = f\"combined_{metric_type}_metrics.csv\"\n","        files = glob.glob(os.path.join(directory, pattern))\n","\n","        if not files:\n","            print(f\"  - No files found for {metric_type}\")\n","            continue\n","\n","        print(f\"  - Found {len(files)} files\")\n","\n","        # List to store dataframes for this metric type\n","        dfs = []\n","\n","        # Process each file\n","        for file in files:\n","            try:\n","                # Read the file with the first column as index (GitHub repo URLs)\n","                df = pd.read_csv(file, index_col=0)\n","\n","                print(f\"    - Reading {os.path.basename(file)} with shape {df.shape}\")\n","\n","                # Add to the list of dataframes\n","                dfs.append(df)\n","            except Exception as e:\n","                print(f\"    - Error reading {file}: {e}\")\n","\n","        if not dfs:\n","            print(f\"  - No valid data found for {metric_type}\")\n","            continue\n","\n","        # Combine all dataframes for this metric type\n","        # This will automatically align the data by index (repo URLs) and columns (dates)\n","        # Missing values will be NaN\n","        combined_df = pd.concat(dfs, axis=1)\n","\n","        # Remove duplicate columns if any\n","        combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n","\n","        # Sort columns (dates) chronologically\n","        try:\n","            combined_df = combined_df.reindex(sorted(combined_df.columns,\n","                                                    key=lambda x: datetime.strptime(x, '%Y-%m')),\n","                                             axis=1)\n","        except Exception as e:\n","            print(f\"    - Warning: Could not sort columns chronologically: {e}\")\n","\n","        # Save the combined file\n","        output_file = os.path.join(output_dir, f\"{metric_type}_combined.csv\")\n","        combined_df.to_csv(output_file)\n","\n","        print(f\"  - Saved combined file to {output_file} with shape {combined_df.shape}\")\n","        print(f\"  - Contains data for {len(combined_df.index)} repositories and {len(combined_df.columns)} date columns\")\n"],"metadata":{"id":"96-yBhanhZiC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the function\n","combine_metric_files()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6aMMNSc4Ops","executionInfo":{"status":"ok","timestamp":1740852393183,"user_tz":-420,"elapsed":68,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"ab99f8d3-1bc9-4702-fd4c-193e5d75fa06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing commits files...\n","  - Found 6 files\n","    - Reading [grace_6months]_commits_metrics.csv with shape (96, 6)\n","    - Reading [grace]_commits_metrics.csv with shape (85, 27)\n","    - Reading [thai]_commits_metrics.csv with shape (92, 27)\n","    - Reading [thai_6months]_commits_metrics.csv with shape (100, 6)\n","    - Reading [pao]_commits_metrics.csv with shape (115, 27)\n","    - Reading [pao_6months]_commits_metrics.csv with shape (96, 6)\n","  - Saved combined file to output/commits_combined.csv with shape (292, 33)\n","  - Contains data for 292 repositories and 33 date columns\n","Processing pull_requests files...\n","  - Found 6 files\n","    - Reading [thai]_pull_requests_metrics.csv with shape (92, 27)\n","    - Reading [thai_6months]_pull_requests_metrics.csv with shape (100, 6)\n","    - Reading [pao]_pull_requests_metrics.csv with shape (115, 27)\n","    - Reading [grace_6months]_pull_requests_metrics.csv with shape (96, 6)\n","    - Reading [pao_6months]_pull_requests_metrics.csv with shape (96, 6)\n","    - Reading [grace]_pull_requests_metrics.csv with shape (85, 27)\n","  - Saved combined file to output/pull_requests_combined.csv with shape (292, 33)\n","  - Contains data for 292 repositories and 33 date columns\n","Processing stars files...\n","  - Found 6 files\n","    - Reading [pao_6months]_stars_metrics.csv with shape (96, 6)\n","    - Reading [thai]_stars_metrics.csv with shape (92, 27)\n","    - Reading [thai_6months]_stars_metrics.csv with shape (100, 6)\n","    - Reading [pao]_stars_metrics.csv with shape (115, 27)\n","    - Reading [grace_6months]_stars_metrics.csv with shape (96, 6)\n","    - Reading [grace]_stars_metrics.csv with shape (85, 27)\n","  - Saved combined file to output/stars_combined.csv with shape (292, 33)\n","  - Contains data for 292 repositories and 33 date columns\n","Processing contributors files...\n","  - Found 6 files\n","    - Reading [grace_6months]_contributors_metrics.csv with shape (96, 6)\n","    - Reading [thai]_contributors_metrics.csv with shape (92, 27)\n","    - Reading [pao_6months]_contributors_metrics.csv with shape (96, 6)\n","    - Reading [pao]_contributors_metrics.csv with shape (115, 27)\n","    - Reading [grace]_contributors_metrics.csv with shape (85, 27)\n","    - Reading [thai_6months]_contributors_metrics.csv with shape (100, 6)\n","  - Saved combined file to output/contributors_combined.csv with shape (292, 33)\n","  - Contains data for 292 repositories and 33 date columns\n","Processing forks files...\n","  - Found 6 files\n","    - Reading [pao]_forks_metrics.csv with shape (115, 27)\n","    - Reading [grace_6months]_forks_metrics.csv with shape (96, 6)\n","    - Reading [grace]_forks_metrics.csv with shape (85, 27)\n","    - Reading [pao_6months]_forks_metrics.csv with shape (96, 6)\n","    - Reading [thai_6months]_forks_metrics.csv with shape (100, 6)\n","    - Reading [thai]_forks_metrics.csv with shape (92, 27)\n","  - Saved combined file to output/forks_combined.csv with shape (292, 33)\n","  - Contains data for 292 repositories and 33 date columns\n"]}]},{"cell_type":"code","source":["# Example of loading and displaying a combined file\n","# Uncomment these lines to see a preview of the combined data\n","\"\"\"\n","# Load a combined file\n","combined_commits = pd.read_csv('output/commits_combined.csv', index_col=0)\n","\n","# Display the first few rows\n","print(\"\\nSample of combined commits data:\")\n","display(combined_commits.head())\n","\n","# Display basic statistics\n","print(\"\\nBasic statistics:\")\n","print(f\"Number of repositories: {len(combined_commits.index)}\")\n","print(f\"Date range: {combined_commits.columns[0]} to {combined_commits.columns[-1]}\")\n","print(f\"Total data points: {combined_commits.count().sum()}\")\n","\"\"\""],"metadata":{"id":"xo4j7EDm413C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# format for RQ3 thai sentiment hackernews submission"],"metadata":{"id":"dzHZrBjUELVk"}},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime, timedelta\n","import os\n","\n","def load_metrics(metrics_dir):\n","    \"\"\"\n","    Load all metric CSVs into a dictionary of DataFrames.\n","\n","    Args:\n","        metrics_dir (str): Directory containing metric CSV files\n","\n","    Returns:\n","        dict: Dictionary with metric names as keys and DataFrames as values\n","    \"\"\"\n","    metrics = {}\n","    metric_files = ['stars', 'commits', 'pull_requests', 'forks', 'contributors']\n","\n","    for metric in metric_files:\n","        file_path = os.path.join(metrics_dir, f'combined_{metric}_metrics.csv')\n","        if os.path.exists(file_path):\n","            metrics[metric] = pd.read_csv(file_path, index_col=0)\n","        else:\n","            print(f\"Warning: {metric}_metrics.csv not found\")\n","\n","    return metrics\n","\n","def unix_to_datetime(unix_timestamp):\n","    \"\"\"Convert Unix timestamp to datetime object.\"\"\"\n","    return datetime.fromtimestamp(int(unix_timestamp))\n","\n","def find_closest_date_column(df, target_date):\n","    \"\"\"\n","    Find the closest date column in the metrics DataFrame to the target date.\n","\n","    Args:\n","        df (pd.DataFrame): Metrics DataFrame\n","        target_date (datetime): Target date to match\n","\n","    Returns:\n","        str: Name of the closest date column\n","    \"\"\"\n","    # Convert column names to datetime objects\n","    date_cols = [datetime.strptime(col, '%Y-%m') for col in df.columns]\n","\n","    # Find the column with the minimum absolute difference\n","    closest_date = min(date_cols, key=lambda x: abs(x - target_date))\n","\n","    # Convert back to original format\n","    return closest_date.strftime('%Y-%m')\n","\n","def get_metric_value(df, repo_url, date_col):\n","    \"\"\"\n","    Get metric value for a specific repository and date.\n","    Returns -1 if data is not available.\n","    \"\"\"\n","    try:\n","        return df.loc[repo_url, date_col]\n","    except:\n","        return -1\n","\n","def process_metrics(hn_data_path, metrics_dir, output_path):\n","    \"\"\"\n","    Process metrics and create consolidated CSV file.\n","\n","    Args:\n","        hn_data_path (str): Path to HackerNews submission data CSV\n","        metrics_dir (str): Directory containing metric CSVs\n","        output_path (str): Path for output CSV file\n","    \"\"\"\n","    # Load HackerNews submission data\n","    hn_data = pd.read_csv(hn_data_path)\n","\n","    # Load all metrics\n","    metrics_dict = load_metrics(metrics_dir)\n","    if not metrics_dict:\n","        raise ValueError(\"No metric files found\")\n","\n","    # Initialize result DataFrame with HackerNews metadata\n","    result_df = hn_data.copy()\n","\n","    # Process each repository\n","    for idx, row in result_df.iterrows():\n","        repo_url = row['url']\n","        submission_date = unix_to_datetime(row['date'])\n","\n","        # Process each metric type\n","        for metric_name, metric_df in metrics_dict.items():\n","            # Get metrics at submission date\n","            closest_submit_date = find_closest_date_column(metric_df, submission_date)\n","            result_df.at[idx, f'{metric_name}_at_submission'] = get_metric_value(\n","                metric_df, repo_url, closest_submit_date)\n","\n","            # Get metrics for each month after submission\n","            for month in range(1, 6):\n","                target_date = submission_date + timedelta(days=30 * month)\n","                closest_month_date = find_closest_date_column(metric_df, target_date)\n","                result_df.at[idx, f'{metric_name}_month_{month}'] = get_metric_value(\n","                    metric_df, repo_url, closest_month_date)\n","\n","    # Save the result\n","    result_df.to_csv(output_path, index=False)\n","    print(f\"Saved consolidated metrics to {output_path}\")\n","\n","    # Print sample of the output\n","    print(\"\\nFirst few rows of the output:\")\n","    print(result_df.head())"],"metadata":{"id":"8yH7k8vPEOF9","executionInfo":{"status":"ok","timestamp":1740855949676,"user_tz":-420,"elapsed":19,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Update these paths according to your file structure\n","HN_DATA_PATH = \"/content/drive/MyDrive/datasets/muict-naist-senior/rq1/rq1_freq_analysis/rq1_stories_github.csv\"\n","METRICS_DIR = \"./\"\n","OUTPUT_PATH = \"consolidated_metrics.csv\"\n","\n","process_metrics(HN_DATA_PATH, METRICS_DIR, OUTPUT_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BvSq28ZeZNW","executionInfo":{"status":"ok","timestamp":1740855954442,"user_tz":-420,"elapsed":2324,"user":{"displayName":"Prachnachai Meakpaiboonwattana","userId":"17764950184050269979"}},"outputId":"262b8cc7-b5b6-476e-daf6-fcd8b4b7f46d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved consolidated metrics to consolidated_metrics.csv\n","\n","First few rows of the output:\n","   discussion_id                                              title  \\\n","0       31355348  BlindAI: Open-source, fast and privacy-friendl...   \n","1       31405976          OpenAI Codex Python to C++ Code Generator   \n","2       31831437                                   Gemini with IPFS   \n","3       31846593  YaLM-100B: Pretrained language model with 100B...   \n","4       32458048  Paradigms of Artificial Intelligence Programmi...   \n","\n","                                           url        date  \\\n","0  https://github.com/mithril-security/blindai  1652368380   \n","1    https://github.com/alxschwrz/codex_py2cpp  1652761021   \n","2     https://github.com/JonStratton/geminipfs  1655865106   \n","3          https://github.com/yandex/YaLM-100B  1655974826   \n","4          https://github.com/norvig/paip-lisp  1660472194   \n","\n","   stars_at_submission  stars_month_1  stars_month_2  stars_month_3  \\\n","0                222.0          236.0          244.0          252.0   \n","1                313.0          332.0          332.0          348.0   \n","2                 20.0           20.0           20.0           20.0   \n","3               2841.0         2895.0         2943.0         2967.0   \n","4               6200.0         6248.0         6282.0         6319.0   \n","\n","   stars_month_4  stars_month_5  ...  forks_month_2  forks_month_3  \\\n","0          263.0          274.0  ...           14.0           14.0   \n","1          358.0          366.0  ...           17.0           20.0   \n","2           20.0           20.0  ...            0.0            0.0   \n","3         3001.0         3065.0  ...          210.0          212.0   \n","4         6348.0         6375.0  ...          626.0          636.0   \n","\n","   forks_month_4  forks_month_5  contributors_at_submission  \\\n","0           16.0           17.0                         0.0   \n","1           21.0           22.0                         1.0   \n","2            0.0            0.0                         3.0   \n","3          216.0          224.0                         3.0   \n","4          637.0          639.0                        43.0   \n","\n","   contributors_month_1  contributors_month_2  contributors_month_3  \\\n","0                   0.0                   0.0                   0.0   \n","1                   1.0                   1.0                   1.0   \n","2                   3.0                   3.0                   3.0   \n","3                   3.0                   3.0                   3.0   \n","4                  43.0                  43.0                  43.0   \n","\n","   contributors_month_4  contributors_month_5  \n","0                   0.0                   2.0  \n","1                   1.0                   1.0  \n","2                   3.0                   3.0  \n","3                   3.0                   3.0  \n","4                  43.0                  43.0  \n","\n","[5 rows x 34 columns]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RS4tm5l7Dv_Z"},"execution_count":null,"outputs":[]}]}