{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comments topic modelling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_KIDS = 100 # per story\n",
    "\n",
    "# HN_STORIES_JSON = '../../data/hn_stories_dataset_final.json' # raw stories data, used to fetch comments from kid ids\n",
    "# HN_COMMENTS_JSON = '../../data/hn_comments_dataset_final.json' # store raw stories + comments\n",
    "# HN_COMMENTS_CSV = '../../data/hn_comments_dataset_final.csv' # converted from json to csv\n",
    "# HN_COMMENTS_SAMPLED_CSV = '../../data/sampled_400_hn_comments_ground_truth.csv' # uniformly sampled comments for ground truth\n",
    "\n",
    "# # Define the base URL for the Hacker News API\n",
    "# BASE_URL = 'https://hacker-news.firebaseio.com/v0'\n",
    "\n",
    "# DEPTH = 1 # comments depth\n",
    "\n",
    "NUM_KIDS = 100 # per story\n",
    "\n",
    "# json's\n",
    "HN_STORIES_JSON = '../../data/hn_stories_dataset_final.json' # contains the raw fetched hn stories and comment ids\n",
    "HN_STORIES_GH_JSON = '../../data/hn_stories_dataset_gh_final.json'\n",
    "HN_COMMENTS_JSON = '../../data/hn_comments_dataset_final.json'\n",
    "HN_COMMENTS_GH_JSON = '../../data/hn_comments_dataset_gh_final.json'\n",
    "\n",
    "# csv's\n",
    "HN_STORIES_CSV = '../../data/hn_stories_dataset_final.csv' # after converting raw json to raw csv\n",
    "HN_STORIES_GH_CSV = '../../data/hn_stories_dataset_gh_final.csv' # after converting raw json to raw csv\n",
    "HN_COMMENTS_CSV = '../../data/hn_comments_dataset_final.csv' # after converting raw json to raw csv\n",
    "HN_COMMENTS_GH_CSV = '../../data/hn_comments_dataset_gh_final.csv' # after converting raw json to raw csv\n",
    "HN_COMMENTS_SAMPLED_CSV = '../../data/hn_comments_dataset_stratified_final.csv' # after converting raw json to raw csv\n",
    "\n",
    "# txt's\n",
    "KEYWORDS_TXT = 'ai_keywords.txt' # used to match relevant hn stories titles\n",
    "HN_GITHUB_URLS_TXT = 'hn_github_urls.txt'\n",
    "\n",
    "# Define the base URL for the Hacker News API\n",
    "BASE_URL = 'https://hacker-news.firebaseio.com/v0'\n",
    "\n",
    "# hackernews stories id's\n",
    "CHATGPT_RELEASE_ID = 33804874 # nov 30th 2022\n",
    "START_ID = 31300000 # may 8th 2022\n",
    "END_ID = 40300000 # may 9th 2024\n",
    "\n",
    "# dataset retrieval parameters\n",
    "INCREMENT = 1 # 1: fetch every stories, 2: skip every other stories, 3: skip every 2 stories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(HN_COMMENTS_GH_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess all comments\n",
    "print(\"Preprocessing comments...\")\n",
    "df['processed_text'] = df['comment_text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Remove empty comments after preprocessing\n",
    "df = df[df['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# Create document-term matrix\n",
    "print(\"Creating document-term matrix...\")\n",
    "vectorizer = CountVectorizer(max_df=0.95,  # Remove terms that appear in >95% of docs\n",
    "                           min_df=2,       # Remove terms that appear in <2 docs\n",
    "                           max_features=5000)\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Function to find optimal number of topics\n",
    "def evaluate_topics(doc_term_matrix, max_topics, random_state=42):\n",
    "    perplexities = []\n",
    "\n",
    "    for n_topics in tqdm(range(2, max_topics + 1)):\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                      random_state=random_state,\n",
    "                                      n_jobs=-1)\n",
    "        lda.fit(doc_term_matrix)\n",
    "        perplexities.append(lda.perplexity(doc_term_matrix))\n",
    "\n",
    "    return perplexities\n",
    "\n",
    "# Find optimal number of topics\n",
    "print(\"Finding optimal number of topics...\")\n",
    "max_topics = 20\n",
    "perplexities = evaluate_topics(doc_term_matrix, max_topics)\n",
    "\n",
    "# Plot perplexity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_topics + 1), perplexities, marker='o')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Perplexity Score')\n",
    "plt.title('Optimal Number of Topics')\n",
    "plt.show()\n",
    "\n",
    "# Train final LDA model with optimal number of topics\n",
    "# Note: You should choose the number of topics based on the perplexity plot\n",
    "# and your domain knowledge. Here we'll use 10 as an example.\n",
    "n_topics = 10\n",
    "print(f\"Training final LDA model with {n_topics} topics...\")\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                     random_state=42,\n",
    "                                     n_jobs=-1)\n",
    "lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Print top terms for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "\n",
    "print(\"\\nTop terms per topic:\")\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-n_top_words-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Create interactive visualization\n",
    "print(\"\\nGenerating interactive visualization...\")\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda_model, doc_term_matrix, vectorizer)\n",
    "pyLDAvis.display(vis)\n",
    "\n",
    "# Add topic labels to original dataframe\n",
    "df['dominant_topic'] = lda_output.argmax(axis=1)\n",
    "\n",
    "# Create time series analysis of topics\n",
    "df['comment_date'] = pd.to_datetime(df['comment_date'])\n",
    "df['comment_month'] = df['comment_date'].dt.to_period('M')\n",
    "\n",
    "# Calculate topic distribution over time\n",
    "topic_time_dist = pd.crosstab(df['comment_month'], df['dominant_topic'])\n",
    "topic_time_dist_norm = topic_time_dist.div(topic_time_dist.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot topic distribution over time\n",
    "plt.figure(figsize=(15, 8))\n",
    "topic_time_dist_norm.plot(kind='area', stacked=True)\n",
    "plt.title('Topic Distribution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion of Topics')\n",
    "plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
